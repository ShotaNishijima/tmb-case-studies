```{r echo=FALSE, eval=TRUE, results='hide',include=FALSE}
library(TMB)
library(tmbstan)
source("../utils.R")
#source("bayesian.R")
```

## **Bayesian analysis with `tmbstan`**
In this example you learn:

* How to set up a Bayesian model (with priors on parameters) in TMB
* Use package `tmbstan` to draw MCMC samples from a TMB model
* Compare integration methods: Laplace approximation versus MCMC on the same model with no additional effort
* Test the accuracy of the Laplace approximation to the marginal likelihood
* Write a C++ function that is called by the TMB objective function

***
#### Description of the practical situation
The model itself is not as important, but it is a binomial GLMM with
three crossed random effects. This particular analysis comes from
the paper Monnahan and Kristensen (2018), where you can find a more
detailed description of the model and analysis, as well as further
resources.

`tmbstan` is an R-package which links `TMB` to the software [Stan](http://mc-stan.org/) which does Hamiltonian Monte Carlo (HMC). With `tmbstan` you can sample from any TMB objective function (then interpreted as an un-normalized posterior). It requires a fully Bayesian perspective, and typically involves putting priors on all parameters. You can still use the Laplace approximation, which opens up for two different configurations:

1. Sample both parameters and latent random variables (the random effects) with HMC
2. Integrate out latent random variables using the Laplace approximation, while sampling parameters with HMC

More details about these options are given below.

</details>
<details> <summary> Comparing Laplace approximation and MCMC</summary>
The Laplace approximation (LA) is 
used by TMB to  approximate the marginal likelihood (i.e., the
likelihood with the random effects integrated out). Typically, there is no
easy way to test how valid the approximation is. Here, we demonstrate how
to do this by integrating the model in the two different ways mentioned above: (1) with MCMC sampling for all parameters, 
and (2) with MCMC for fixed effects but LA for
random effects. That is, the MCMC algorithm samples from the approximated
marginal posterior. If the LA is accurate, then there will be no difference
in the posterior distribution for the *fixed effects* (parameters). 
We cannot directly compare the random effects.

Thus, if the distribution is the same we have confidence that the LA is
accurate. If not, this suggests an inaccuracy in the LA and caution should
be taken when using it for inference. In the case of this model, we quickly
see that the LA version produces very different results for the fixed
effects.

Another interesting note is that while the LA version mixes better in the
sense that it produces higher effective samples, it takes much longer to
run and thus the full MCMC integration should be prefered from an
efficiency standpoint as well (see table S2 in Monnahan and Kristensen
2018).
</details>

***
#### Full code for example
<details> <summary> R code</summary>
```{r,echo=FALSE, comment=""}
include_source("bayesian.R", linesToInclud=c(1:3, 10:999))
```
</details>
<details> <summary> C++ code</summary>
```{r,echo=FALSE, comment=""}
include_source("bayesian.cpp", linesToInclud=c(1:999))
```
</details>

***
#### C++ code step-by-step

<details>
  <summary>Writing a C++ function for the Cauchy density</summary>

* The Cauchy density is not available in TMB. The density is given as: 
  $$ f(x;mean,\gamma) = \frac{1}{\pi\gamma\left[\left(\frac{x-mean}{\gamma}\right)\right]}$$

* The following code implements $f(x;mean,\gamma)$, and serves as an example of how you can write your own C++ functions:
```c++
    template<class Type>   // See explanation below
    Type dcauchy(Type x, Type mean, Type gamma, int give_log=0){  // Function body starts here
    Type logres = 0.0;    // Define variable that holds return value
    logres -= log(M_PI);   // Subtract log("pi")
    logres -= log(gamma);  // Subtract log("gamma")
    logres -= log(1 + pow( (x-mean)/gamma ,2));  // Main part of log-density
    if(give_log) return logres; else return exp(logres);  // Return value (log or not)
}
```
  * The first code line `template<class Type>` gives the data type returned by the function. In this case it is `Type`, which is the TMB scalar type. 
  * Now, `dcauchy()` can be used as any built in density in the objective function
```c++
  nlp -= dcauchy(yearInterceptSD2, Type(0), Type(5),true);
```
</details>

<details>
  <summary>The objective function (return value) in Bayesian models</summary>

  * In a fully Bayesian model the return value should be 
  the negative log (un-normalized) posterior density. 

  * The return value has to parts: i) contributions from parameter log-priors (stored in variable `nlp`) and log-likelihood contributions (stored in variable `nll`). At the end of the C++ program we return `nlp+nll`.
```c++
  Type nlp=0.0; // negative log prior
  Type nll=0.0; // negative log likelihood
```
</details>

<details>
  <summary>Parameter transformation and Jacobinan correction</summary>

  * As usual we use a log-parameterization of standard deviations. 
    This has the advantage that the sampling can be done on
    the entire real line (as opposed to only the positive part). 
    Back transform to absolute scale:
```c++
  Type yearInterceptSD2=exp(yearInterceptSD);
  Type plantInterceptSD2=exp(plantInterceptSD);
  Type plantSlopeSD2=exp(plantSlopeSD);
```

  * However, because the parameters are random variables in the Bayesian world, we need to add (subtract) a "Jacobian correction" to the objective function according to the [change-of-variable formula](https://en.wikipedia.org/wiki/Probability_density_function#Dependent_variables_and_change_of_variables): 
```c++
  nll -= yearInterceptSD + plantInterceptSD + plantSlopeSD;
```
See excercise below for more details.

</details>


<details>
  <summary>Setting priors on parameters</summary>

  * As noted above, a fully  Bayesian model requires priors on all parameters (fixed effects, variances, correlations, etc.)

  * This can be done explicitely:
```c++
  nlp -= dcauchy(yearInterceptSD2, Type(0), Type(5), true);
  nlp -= dcauchy(plantInterceptSD2, Type(0), Type(5), true);
  nlp -= dcauchy(plantSlopeSD2, Type(0), Type(5), true);
  nlp -= dnorm(slope, Type(0.0), Type(10.0), true);
  nlp -= dnorm(intercept, Type(0.0), Type(10.0), true).sum();
```

* If you omit a parameter from this list, it will amount to giving
it a flat (improper) prior, but
see [Stan-Prior-Choice Recommendations](https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations)
</details>

<details>
  <summary>Log likelihood</summary>

* Contribution from the binomial likelihood of the reponse:

```c++
  Type ypred;
  // model predictions
  for(int i=0; i<Ndata; i++){
    // prediction logit scale
    ypred= intercept(stage(i)-1) +
      yearInterceptEffect_raw(year(i)-1)*yearInterceptSD2 +
      plantInterceptEffect_raw(plant(i)-1)*plantInterceptSD2+
      Pods(i) * plantSlopeEffect_raw(plant(i)-1)*plantSlopeSD2+
      Pods(i) * slope;
    // likelihood contribution
    if(toF(i)==1){
      nll += log(1+exp(-ypred));
    } else {
      nll += ypred+log(1+exp(-ypred));
    }
  }
```

* Contribution from random effects (which may have been included instead in `nlp`) 

```c++
  // random effects; non-centered
  nll -= dnorm(yearInterceptEffect_raw, Type(0.0), Type(1.0), true).sum();
  nll -= dnorm(plantInterceptEffect_raw,Type(0.0), Type(1.0), true).sum();
  nll -= dnorm(plantSlopeEffect_raw, Type(0.0), Type(1.0), true).sum();

```

</details>

***
#### R code step-by-step

<details>
  <summary>Set up spline structure by using `mgcv`</summary>
```r
  gam_setup = gam(Richness ~ s(ROCK, bs = "cs") +
        s(LITTER, bs = "cs") + s(BARESOIL, bs = "cs") +
        s(FallPrec, bs = "cs") + s(SprTmax, bs = "cs"),
      data = Vegetation,fit=FALSE)
```
* `fit=FALSE` is because you only want `mgcv` to set up the spline bases,
   not fit the model.
</details>

***
#### Exercises

1. Explain why the "Jacobian correction" takes the particular form:
```c++
  nll -= yearInterceptSD + plantInterceptSD + plantSlopeSD;
```
<details>
  <summary>Solution</summary>
In the notation of [change-of-variable formula](https://en.wikipedia.org/wiki/Probability_density_function#Dependent_variables_and_change_of_variables)
we set $x=\sigma$ and $y=\log\sigma$, where $\sigma$ is the standard
deviation of the prior. Hence, 
$g(x)=\log(x)$ and $g^{-1}(y)=\exp(y)$, and the Jacobian determinant becomes
$$
|J|=\left| \frac{d}{dy} \big(g^{-1}(y)\big) \right| = \exp(y),
$$
so we should add (subtract) $\log|J|=y=\log(\sigma)$ to the objective function
for each of the three parameters.
</details>


***
#### References
Monnahan, C. C. and K. Kristensen. 2018. No-U-turn sampling for fast
Bayesian inference in ADMB and TMB: Introducing the adnuts and tmbstan R
packages. Plos One 13:e0197954.
